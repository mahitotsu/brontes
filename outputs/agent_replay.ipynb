{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5356843",
   "metadata": {},
   "source": [
    "# Agent Execution Replay\n",
    "\n",
    "このノートブックは、Strands Agentが実行したPythonコードを再現可能な形で記録しています。\n",
    "\n",
    "## 実行概要\n",
    "\n",
    "- **Model**: `jp.anthropic.claude-haiku-4-5-20251001-v1:0`\n",
    "- **抽出されたコードブロック数**: 9\n",
    "- **トレース元**: `outputs/trace.jsonl`\n",
    "\n",
    "## トレースの仕組み\n",
    "\n",
    "Strands AgentsはOpenTelemetryを使用して実行トレースを記録します。トレースには以下の情報が含まれます：\n",
    "\n",
    "- LLMへのプロンプトとレスポンス\n",
    "- 使用されたツール（python_repl）とそのパラメータ\n",
    "- 実行時間とトークン使用量\n",
    "- 各サイクルの処理フロー"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520e623",
   "metadata": {},
   "source": [
    "## システムプロンプト\n",
    "\n",
    "Agentに与えられた役割と責務：\n",
    "\n",
    "```\n",
    "## タスク\n",
    "指定された条件に基づいて、XMLファイルからAWSニュース記事を抽出する。\n",
    "\n",
    "## データソース\n",
    "- **パス**: `data/01_raw/*.xml`\n",
    "- **形式**: RSS 2.0 (各 `<item>` が1記事)\n",
    "- **フィールド**:\n",
    "  - `<guid>`: 記事ID\n",
    "  - `<title>`: タイトル\n",
    "  - `<pubDate>`: 公開日時（RFC 2822形式）\n",
    "  - `<description>`: 記事本文\n",
    "  - `<link>`: URL\n",
    "\n",
    "## 入力条件\n",
    "- **期間**: 開始日時 ≤ pubDate ≤ 終了日時\n",
    "- **トピックキーワード**: リスト形式で提供\n",
    "- **リージョンキーワード**: リスト形式で提供\n",
    "\n",
    "## 出力形式\n",
    "```json\n",
    "{\n",
    "  \"articles\": [\n",
    "    {\n",
    "      \"guid\": \"記事ID\",\n",
    "      \"title\": \"タイトル\",\n",
    "      \"pubDate\": \"ISO 8601形式の日時\",\n",
    "      \"description\": \"本文\",\n",
    "      \"link\": \"URL\"\n",
    "    }\n",
    "  ],\n",
    "  \"summary\": {\n",
    "    \"total_count\": 総記事数,\n",
    "    \"filtered_count\": フィルタ後記事数,\n",
    "    \"from_date\": \"ISO 8601形式\",\n",
    "    \"to_date\": \"ISO 8601形式\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 抽出条件\n",
    "1. **期間**: 指定された期間内に公開された記事\n",
    "2. **キーワード**: title または description に以下を含む\n",
    "   - トピックキーワードから1つ以上 **かつ**\n",
    "   - リージョンキーワードから1つ以上\n",
    "   - 大文字小文字を区別しない\n",
    "3. **重複排除**: guid でユニーク化\n",
    "4. **ソート**: pubDate 降順（新しい順）\n",
    "\n",
    "## 制約\n",
    "- 利用可能なツール：python_repl のみ\n",
    "- 最終結果は再利用可能な単一の関数で取得すること\n",
    "- 関数内で値をハードコードしない（全てパラメータで受け取る）\n",
    "- グローバル変数に依存しない\n",
    "- ファイル出力しない\n",
    "\n",
    "## コメント要件\n",
    "- 探索に使用するPythonコードには、処理の意図や確認内容を説明するコメントを含めること\n",
    "- 最終関数には、引数・戻り値・処理フローを説明する適切なdocstringとコメントを含めること\n",
    "- コメントは必要十分な情報を簡潔に記述すること\n",
    "\n",
    "## 実装フロー\n",
    "- Pythonコードは作成して実行するのみ\n",
    "- 追加の説明は不要（コード内のコメント・docstringで十分）\n",
    "- **重要**: 実装仕様の確認や説明用のPythonコード（print文で仕様表示など）は作成しない\n",
    "- 探索と最終関数取得のみに集中すること\n",
    "\n",
    "## 技術的要件\n",
    "最終関数の直前に実装の設計判断とその根拠を説明するコメントブロックを出力し、その後に最終関数を出力すること。さらに、最終関数を呼び出して結果を取得するコードも出力すること。以下のマーカー形式を使用（トレースからの自動抽出のため）：\n",
    "\n",
    "```python\n",
    "# === IMPLEMENTATION_RATIONALE_START ===\n",
    "# [この実装に至った設計判断とその根拠を簡潔に説明]\n",
    "# 例：\n",
    "# - ライブラリ選択: ○○を使用（理由：性能/標準ライブラリ/可読性など）\n",
    "# - データ構造: ○○を選択（理由：検索効率/メモリ効率など）\n",
    "# - アルゴリズム: ○○の順序で処理（理由：効率性/正確性など）\n",
    "# - エラー処理: ○○の方針（理由：堅牢性/デバッグ容易性など）\n",
    "# === IMPLEMENTATION_RATIONALE_END ===\n",
    "\n",
    "# === FINAL_FUNCTION_START ===\n",
    "# 必要な全てのimport文を含める\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def 関数名(...):\n",
    "    \"\"\"\n",
    "    関数の説明\n",
    "    \n",
    "    Args:\n",
    "        引数の説明\n",
    "    \n",
    "    Returns:\n",
    "        戻り値の説明\n",
    "    \"\"\"\n",
    "    # 処理の説明コメント\n",
    "    ...\n",
    "# === FINAL_FUNCTION_END ===\n",
    "\n",
    "# === FUNCTION_CALL_START ===\n",
    "# パラメータの設定（必要に応じて外部変数として定義）\n",
    "data_dir_path = Path('data/01_raw')\n",
    "from_date = datetime(2025, 7, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "to_date = datetime(2025, 12, 31, 23, 59, 59, tzinfo=timezone.utc)\n",
    "topic_keywords = ['Bedrock', 'SageMaker', ...]\n",
    "region_keywords = ['Tokyo', 'ap-northeast-1', ...]\n",
    "\n",
    "# 関数の実行\n",
    "result = 関数名(\n",
    "    data_dir=data_dir_path,\n",
    "    from_date=from_date,\n",
    "    to_date=to_date,\n",
    "    topic_keywords=topic_keywords,\n",
    "    region_keywords=region_keywords,\n",
    ")\n",
    "print(result)\n",
    "# === FUNCTION_CALL_END ===\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce8696",
   "metadata": {},
   "source": [
    "## ユーザーリクエスト\n",
    "\n",
    "Agentに与えられたタスク：\n",
    "\n",
    "```\n",
    "以下の条件でAWSニュース記事をフィルタリングしてください：\n",
    "**期間**: 2025年下半期（7月1日 00:00:00 ～ 12月31日 23:59:59）\n",
    "**トピック**: 生成AI関連のアナウンスと更新で東京リージョンで利用可能なもの\n",
    "\n",
    "**キーワード**:\n",
    "- 生成AI関連: Bedrock, SageMaker, Claude, LLM, foundation model, generative AI, embedding, fine-tuning, inference, RAG, prompt, model training, neural network, transformer, deep learning, machine learning\n",
    "- 東京リージョン関連: Tokyo, ap-northeast-1, Asia Pacific (Tokyo), available in Tokyo, Tokyo region, all regions, all aws regions,\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc60c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Agentの思考プロセスと実行履歴\n",
    "\n",
    "以下は、Agentがタスクを実行した際の思考プロセスと実行コードを時系列で記録したものです。\n",
    "\n",
    "※このセクションは参照用です。実際の実行は「最終成果物」セクションで行ってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca5968",
   "metadata": {},
   "source": [
    "## Step 1: Agent の思考\n",
    "\n",
    "XMLファイルを探索して、指定された条件に基づいてフィルタリングを実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320055a9",
   "metadata": {},
   "source": [
    "### 実行コード (Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9c626",
   "metadata": {
    "tags": [
     "reference-only"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# データディレクトリを確認\n",
    "data_dir = Path('data/01_raw')\n",
    "xml_files = list(data_dir.glob('*.xml'))\n",
    "print(f\"Found {len(xml_files)} XML files:\")\n",
    "for f in xml_files[:5]:  # 最初の5つを表示\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8adc6",
   "metadata": {},
   "source": [
    "### 実行コード (Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f2e8b",
   "metadata": {
    "tags": [
     "reference-only"
    ]
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# XMLファイルの構造を確認\n",
    "xml_file = xml_files[0]\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "# ルート要素を確認\n",
    "print(f\"Root tag: {root.tag}\")\n",
    "print(f\"Root attribs: {root.attrib}\")\n",
    "\n",
    "# 最初のitemを確認して構造を理解\n",
    "for item in root.findall('.//item')[:1]:\n",
    "    print(\"\\nFirst item structure:\")\n",
    "    for child in item:\n",
    "        text_preview = child.text[:100] if child.text else \"(empty)\"\n",
    "        print(f\"  <{child.tag}>: {text_preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68e7c9",
   "metadata": {},
   "source": [
    "### 実行コード (Step 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff46e3",
   "metadata": {
    "tags": [
     "reference-only"
    ]
   },
   "outputs": [],
   "source": [
    "from email.utils import parsedate_to_datetime\n",
    "\n",
    "# pubDate形式の確認と日付パース\n",
    "sample_date_str = \"Thu, 29 Jan 2026 18:30:00 GMT\"\n",
    "parsed_date = parsedate_to_datetime(sample_date_str)\n",
    "print(f\"Original: {sample_date_str}\")\n",
    "print(f\"Parsed: {parsed_date}\")\n",
    "print(f\"ISO format: {parsed_date.isoformat()}\")\n",
    "\n",
    "# 対象期間を確認\n",
    "from datetime import datetime, timezone\n",
    "from_date = datetime(2025, 7, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "to_date = datetime(2025, 12, 31, 23, 59, 59, tzinfo=timezone.utc)\n",
    "print(f\"\\nTarget period: {from_date} to {to_date}\")\n",
    "print(f\"Sample date in range? {from_date <= parsed_date <= to_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1845dd",
   "metadata": {},
   "source": [
    "### 実行コード (Step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed965ce",
   "metadata": {
    "tags": [
     "reference-only"
    ]
   },
   "outputs": [],
   "source": [
    "# すべてのファイルから記事の日付範囲を確認\n",
    "all_dates = []\n",
    "for xml_file in xml_files:\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    for item in root.findall('.//item'):\n",
    "        pub_date_elem = item.find('pubDate')\n",
    "        if pub_date_elem is not None and pub_date_elem.text:\n",
    "            parsed_date = parsedate_to_datetime(pub_date_elem.text)\n",
    "            all_dates.append(parsed_date)\n",
    "\n",
    "all_dates.sort()\n",
    "print(f\"Total articles: {len(all_dates)}\")\n",
    "if all_dates:\n",
    "    print(f\"Date range: {all_dates[0]} to {all_dates[-1]}\")\n",
    "    \n",
    "    # 2025年下半期のデータを確認\n",
    "    july_2025_start = datetime(2025, 7, 1, tzinfo=timezone.utc)\n",
    "    dec_2025_end = datetime(2025, 12, 31, 23, 59, 59, tzinfo=timezone.utc)\n",
    "    \n",
    "    articles_in_range = [d for d in all_dates if july_2025_start <= d <= dec_2025_end]\n",
    "    print(f\"Articles in 2025 H2 (Jul-Dec): {len(articles_in_range)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc6e80",
   "metadata": {},
   "source": [
    "### 実行コード (Step 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f4f31",
   "metadata": {
    "tags": [
     "reference-only"
    ]
   },
   "outputs": [],
   "source": [
    "# キーワードマッチング機能をテスト\n",
    "sample_title = \"Announcing AWS Bedrock Claude 3 in Asia Pacific (Tokyo)\"\n",
    "sample_desc = \"New foundation model fine-tuning and RAG capabilities available in all AWS regions including ap-northeast-1\"\n",
    "\n",
    "topic_keywords = [\n",
    "    'Bedrock', 'SageMaker', 'Claude', 'LLM', 'foundation model', 'generative AI', \n",
    "    'embedding', 'fine-tuning', 'inference', 'RAG', 'prompt', 'model training', \n",
    "    'neural network', 'transformer', 'deep learning', 'machine learning'\n",
    "]\n",
    "region_keywords = [\n",
    "    'Tokyo', 'ap-northeast-1', 'Asia Pacific (Tokyo)', 'available in Tokyo', \n",
    "    'Tokyo region', 'all regions', 'all aws regions'\n",
    "]\n",
    "\n",
    "def check_keywords(text, keywords):\n",
    "    \"\"\"大文字小文字を区別せずにキーワードをチェック\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    text_lower = text.lower()\n",
    "    found = [kw for kw in keywords if kw.lower() in text_lower]\n",
    "    return found\n",
    "\n",
    "# テスト\n",
    "combined = f\"{sample_title} {sample_desc}\"\n",
    "topic_found = check_keywords(combined, topic_keywords)\n",
    "region_found = check_keywords(combined, region_keywords)\n",
    "\n",
    "print(f\"Title: {sample_title}\")\n",
    "print(f\"Description: {sample_desc[:100]}...\")\n",
    "print(f\"\\nTopic keywords found: {topic_found}\")\n",
    "print(f\"Region keywords found: {region_found}\")\n",
    "print(f\"Match both? {len(topic_found) > 0 and len(region_found) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060016b",
   "metadata": {},
   "source": [
    "### 実行コード (Step 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a454d",
   "metadata": {
    "tags": [
     "reference-only"
    ]
   },
   "outputs": [],
   "source": [
    "# 対象期間内の実際の記事を確認してマッチングテスト\n",
    "from_date = datetime(2025, 7, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "to_date = datetime(2025, 12, 31, 23, 59, 59, tzinfo=timezone.utc)\n",
    "\n",
    "# 対象期間内の記事をサンプル取得\n",
    "sample_articles = []\n",
    "for xml_file in xml_files:\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    for item in root.findall('.//item'):\n",
    "        pub_date_elem = item.find('pubDate')\n",
    "        title_elem = item.find('title')\n",
    "        \n",
    "        if pub_date_elem is not None and pub_date_elem.text:\n",
    "            parsed_date = parsedate_to_datetime(pub_date_elem.text)\n",
    "            if from_date <= parsed_date <= to_date:\n",
    "                title = title_elem.text if title_elem is not None else \"\"\n",
    "                sample_articles.append({\n",
    "                    'title': title,\n",
    "                    'pubDate': parsed_date.isoformat(),\n",
    "                    'parsed_date': parsed_date\n",
    "                })\n",
    "\n",
    "# 日付でソート\n",
    "sample_articles.sort(key=lambda x: x['parsed_date'], reverse=True)\n",
    "\n",
    "print(f\"Sample articles in H2 2025 (first 5):\")\n",
    "for i, article in enumerate(sample_articles[:5]):\n",
    "    print(f\"\\n{i+1}. [{article['pubDate']}]\")\n",
    "    print(f\"   {article['title'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b77a9",
   "metadata": {},
   "source": [
    "### 実行コード (Step 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b800f",
   "metadata": {
    "tags": [
     "reference-only"
    ]
   },
   "outputs": [],
   "source": [
    "# 実装設計の確認：複数ファイル対応、デリケートな処理順序、エラーハンドリング\n",
    "\n",
    "# === IMPLEMENTATION_RATIONALE_START ===\n",
    "# - ライブラリ選択: xml.etree.ElementTree（標準ライブラリ、RSS/XMLパース標準）\n",
    "#   email.utils.parsedate_to_datetime（RFC 2822日付パース標準）\n",
    "#   json（出力フォーマット標準）\n",
    "# - データ構造: dict/list（JSONシリアライズ対応、メモリ効率）\n",
    "#   set でguid重複排除後、list に戻す（順序保証）\n",
    "# - アルゴリズム処理順序:\n",
    "#   1. XMLファイル全体をパース（複数ファイル対応）\n",
    "#   2. 日付フィルタ（効率的な絞り込み）\n",
    "#   3. キーワード照合（トピック AND リージョン条件）\n",
    "#   4. guid による重複排除\n",
    "#   5. pubDate 降順ソート\n",
    "# - エラー処理: 不正な日付形式・空要素を安全にスキップ\n",
    "#   XMLパースエラーで該当ファイルスキップ、処理継続\n",
    "# === IMPLEMENTATION_RATIONALE_END ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a059653",
   "metadata": {},
   "source": [
    "### 実行コード (Step 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd70e7",
   "metadata": {
    "tags": [
     "reference-only"
    ]
   },
   "outputs": [],
   "source": [
    "# === FINAL_FUNCTION_START ===\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from email.utils import parsedate_to_datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def extract_aws_articles(\n",
    "    data_dir: Path,\n",
    "    from_date: datetime,\n",
    "    to_date: datetime,\n",
    "    topic_keywords: List[str],\n",
    "    region_keywords: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    XMLファイルからAWSニュース記事をフィルタリングして抽出する。\n",
    "    \n",
    "    指定された期間内に公開され、トピックキーワードとリージョンキーワードの両方を\n",
    "    含む記事を抽出し、guid でユニーク化してソートする。\n",
    "    \n",
    "    Args:\n",
    "        data_dir: XMLファイルを含むディレクトリパス\n",
    "        from_date: 抽出期間の開始日時 (datetime with timezone)\n",
    "        to_date: 抽出期間の終了日時 (datetime with timezone)\n",
    "        topic_keywords: トピック検索キーワードリスト（いずれか1つ以上マッチ）\n",
    "        region_keywords: リージョン検索キーワードリスト（いずれか1つ以上マッチ）\n",
    "    \n",
    "    Returns:\n",
    "        以下の構造を持つ辞書:\n",
    "        {\n",
    "            \"articles\": [\n",
    "                {\n",
    "                    \"guid\": \"記事ID\",\n",
    "                    \"title\": \"タイトル\",\n",
    "                    \"pubDate\": \"ISO 8601形式の日時\",\n",
    "                    \"description\": \"本文\",\n",
    "                    \"link\": \"URL\"\n",
    "                }\n",
    "            ],\n",
    "            \"summary\": {\n",
    "                \"total_count\": 総記事数,\n",
    "                \"filtered_count\": フィルタ後記事数,\n",
    "                \"from_date\": \"ISO 8601形式\",\n",
    "                \"to_date\": \"ISO 8601形式\"\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # ヘルパー関数: キーワード照合（大文字小文字を区別しない）\n",
    "    def contains_keyword(text: str, keywords: List[str]) -> bool:\n",
    "        \"\"\"テキストにキーワードリストからの1つ以上が含まれるかチェック\"\"\"\n",
    "        if not text:\n",
    "            return False\n",
    "        text_lower = text.lower()\n",
    "        return any(kw.lower() in text_lower for kw in keywords)\n",
    "    \n",
    "    # ヘルパー関数: RFC 2822日付をISO 8601に変換\n",
    "    def parse_pub_date(date_str: str) -> tuple:\n",
    "        \"\"\"\n",
    "        RFC 2822形式の日付文字列をパースし、datetime と ISO文字列のペアを返す。\n",
    "        パース失敗時は (None, None) を返す。\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dt = parsedate_to_datetime(date_str)\n",
    "            return dt, dt.isoformat()\n",
    "        except (TypeError, ValueError):\n",
    "            return None, None\n",
    "    \n",
    "    # XMLファイルを取得（複数ファイル対応）\n",
    "    xml_files = sorted(data_dir.glob('*.xml'))\n",
    "    total_count = 0\n",
    "    articles_dict = {}  # guid をキーとした重複排除用辞書\n",
    "    \n",
    "    # 各XMLファイルをパース\n",
    "    for xml_file in xml_files:\n",
    "        try:\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # RSS 2.0の<item>要素を抽出\n",
    "            for item in root.findall('.//item'):\n",
    "                # 必須フィールドを取得\n",
    "                guid_elem = item.find('guid')\n",
    "                title_elem = item.find('title')\n",
    "                pub_date_elem = item.find('pubDate')\n",
    "                desc_elem = item.find('description')\n",
    "                link_elem = item.find('link')\n",
    "                \n",
    "                # 基本的なデータ抽出\n",
    "                guid = guid_elem.text if guid_elem is not None else None\n",
    "                title = title_elem.text if title_elem is not None else \"\"\n",
    "                description = desc_elem.text if desc_elem is not None else \"\"\n",
    "                link = link_elem.text.strip() if link_elem is not None and link_elem.text else \"\"\n",
    "                \n",
    "                total_count += 1\n",
    "                \n",
    "                # guid がない場合はスキップ\n",
    "                if not guid:\n",
    "                    continue\n",
    "                \n",
    "                # 日付をパースして期間フィルタ\n",
    "                pub_dt, pub_date_iso = parse_pub_date(\n",
    "                    pub_date_elem.text if pub_date_elem is not None else \"\"\n",
    "                )\n",
    "                if pub_dt is None or not (from_date <= pub_dt <= to_date):\n",
    "                    continue\n",
    "                \n",
    "                # キーワード照合：トピック AND リージョン\n",
    "                combined_text = f\"{title} {description}\"\n",
    "                if not contains_keyword(combined_text, topic_keywords):\n",
    "                    continue\n",
    "                if not contains_keyword(combined_text, region_keywords):\n",
    "                    continue\n",
    "                \n",
    "                # guid でユニーク化（重複時は後出を保持）\n",
    "                articles_dict[guid] = {\n",
    "                    \"guid\": guid,\n",
    "                    \"title\": title,\n",
    "                    \"pubDate\": pub_date_iso,\n",
    "                    \"description\": description,\n",
    "                    \"link\": link\n",
    "                }\n",
    "                \n",
    "        except ET.ParseError:\n",
    "            # XMLパースエラーは無視して次ファイルへ\n",
    "            continue\n",
    "    \n",
    "    # pubDate 降順（新しい順）でソート\n",
    "    articles_list = sorted(\n",
    "        articles_dict.values(),\n",
    "        key=lambda x: x[\"pubDate\"],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # 結果を構築\n",
    "    result = {\n",
    "        \"articles\": articles_list,\n",
    "        \"summary\": {\n",
    "            \"total_count\": total_count,\n",
    "            \"filtered_count\": len(articles_list),\n",
    "            \"from_date\": from_date.isoformat(),\n",
    "            \"to_date\": to_date.isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# === FINAL_FUNCTION_END ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb75c7",
   "metadata": {},
   "source": [
    "### 実行コード (Step 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec93148",
   "metadata": {
    "tags": [
     "reference-only"
    ]
   },
   "outputs": [],
   "source": [
    "# === FUNCTION_CALL_START ===\n",
    "# パラメータ設定\n",
    "data_dir_path = Path('data/01_raw')\n",
    "from_date = datetime(2025, 7, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "to_date = datetime(2025, 12, 31, 23, 59, 59, tzinfo=timezone.utc)\n",
    "\n",
    "topic_keywords = [\n",
    "    'Bedrock', 'SageMaker', 'Claude', 'LLM', 'foundation model', 'generative AI',\n",
    "    'embedding', 'fine-tuning', 'inference', 'RAG', 'prompt', 'model training',\n",
    "    'neural network', 'transformer', 'deep learning', 'machine learning'\n",
    "]\n",
    "\n",
    "region_keywords = [\n",
    "    'Tokyo', 'ap-northeast-1', 'Asia Pacific (Tokyo)', 'available in Tokyo',\n",
    "    'Tokyo region', 'all regions', 'all aws regions'\n",
    "]\n",
    "\n",
    "# 関数実行\n",
    "result = extract_aws_articles(\n",
    "    data_dir=data_dir_path,\n",
    "    from_date=from_date,\n",
    "    to_date=to_date,\n",
    "    topic_keywords=topic_keywords,\n",
    "    region_keywords=region_keywords\n",
    ")\n",
    "\n",
    "# 結果を JSON 形式で出力\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "# === FUNCTION_CALL_END ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34688cf5",
   "metadata": {},
   "source": [
    "## Step 10: Agent の思考\n",
    "\n",
    "実装完了しました。\n",
    "\n",
    "## 処理概要\n",
    "\n",
    "**抽出結果**:\n",
    "- **総記事数**: 200件\n",
    "- **フィルタ後**: 6件\n",
    "- **期間**: 2025年7月1日～12月31日\n",
    "\n",
    "## 抽出されたAWSニュース記事（6件）\n",
    "\n",
    "1. **Amazon Connect: 生成AI活用の自動パフォーマンス評価が5言語対応** (2025-12-26)\n",
    "   - キーワード: 生成AI、Asia Pacific (Tokyo)\n",
    "   - アクティブな複数リージョン対応\n",
    "\n",
    "2. **NVIDIA Nemotron 3 Nano on Amazon Bedrock** (2025-12-23)\n",
    "   - キーワード: Bedrock、推論、Asia Pacific (Tokyo)\n",
    "   - 新しい分散推論エンジン「Project Mantle」搭載\n",
    "\n",
    "3. **MiniMax-M2 on Amazon SageMaker JumpStart** (2025-12-23)\n",
    "   - キーワード: SageMaker、foundation model、Asia Pacific (Tokyo)\n",
    "   - 効率的なMoEモデル（230B/10B活性パラメータ）\n",
    "\n",
    "4. **AWS End User Messaging: 生成AI登録レビューア** (2025-12-23)\n",
    "   - キーワード: 生成AI、all regions\n",
    "   - プレビュー機能\n",
    "\n",
    "5. **AWS Storage Gateway: Nutanix AHV対応** (2025-12-22)\n",
    "   - キーワード: machine learning (推奨文脈)、all regions\n",
    "   - ハイブリッドストレージ対応\n",
    "\n",
    "6. **Amazon SageMaker Studio: SOCI高速化対応** (2025-12-19)\n",
    "   - キーワード: SageMaker、machine learning\n",
    "   - コンテナ起動時間30-50%削減\n",
    "\n",
    "## 実装の特徴\n",
    "\n",
    "- **複数ファイル対応**: 複数XMLファイルを自動集約\n",
    "- **安全なパース**: XMLエラーも個別スキップで継続処理\n",
    "- **正確な日付変換**: RFC 2822 → ISO 8601形式へ\n",
    "- **重複排除**: guid をキーとした自動排除\n",
    "- **厳密なフィルタリング**: トピック AND リージョン両キーワード必須\n",
    "- **再利用可能**: 全てパラメータ化、グローバル変数非依存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d97753",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 最終成果物\n",
    "\n",
    "Agentの探索フェーズで得られた知見をもとに、再利用可能な関数として実装しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2bdd20",
   "metadata": {},
   "source": [
    "## 1. 関数定義\n",
    "\n",
    "トレースから抽出した再利用可能な関数です。\n",
    "\n",
    "- 抽出方法: マーカーベース (`# === FINAL_FUNCTION_START/END ===`)\n",
    "- ソースコードブロック数: 9\n",
    "- 抽出元ブロック: 8\n",
    "- import文数: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb23e05c",
   "metadata": {},
   "source": [
    "### 実装の設計判断と根拠\n",
    "\n",
    "- ライブラリ選択: xml.etree.ElementTree（標準ライブラリ、RSS/XMLパース標準）\n",
    "  email.utils.parsedate_to_datetime（RFC 2822日付パース標準）\n",
    "  json（出力フォーマット標準）\n",
    "- データ構造: dict/list（JSONシリアライズ対応、メモリ効率）\n",
    "  set でguid重複排除後、list に戻す（順序保証）\n",
    "- アルゴリズム処理順序:\n",
    "  1. XMLファイル全体をパース（複数ファイル対応）\n",
    "  2. 日付フィルタ（効率的な絞り込み）\n",
    "  3. キーワード照合（トピック AND リージョン条件）\n",
    "  4. guid による重複排除\n",
    "  5. pubDate 降順ソート\n",
    "- エラー処理: 不正な日付形式・空要素を安全にスキップ\n",
    "  XMLパースエラーで該当ファイルスキップ、処理継続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18109be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from email.utils import parsedate_to_datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def extract_aws_articles(\n",
    "    data_dir: Path,\n",
    "    from_date: datetime,\n",
    "    to_date: datetime,\n",
    "    topic_keywords: List[str],\n",
    "    region_keywords: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    XMLファイルからAWSニュース記事をフィルタリングして抽出する。\n",
    "    \n",
    "    指定された期間内に公開され、トピックキーワードとリージョンキーワードの両方を\n",
    "    含む記事を抽出し、guid でユニーク化してソートする。\n",
    "    \n",
    "    Args:\n",
    "        data_dir: XMLファイルを含むディレクトリパス\n",
    "        from_date: 抽出期間の開始日時 (datetime with timezone)\n",
    "        to_date: 抽出期間の終了日時 (datetime with timezone)\n",
    "        topic_keywords: トピック検索キーワードリスト（いずれか1つ以上マッチ）\n",
    "        region_keywords: リージョン検索キーワードリスト（いずれか1つ以上マッチ）\n",
    "    \n",
    "    Returns:\n",
    "        以下の構造を持つ辞書:\n",
    "        {\n",
    "            \"articles\": [\n",
    "                {\n",
    "                    \"guid\": \"記事ID\",\n",
    "                    \"title\": \"タイトル\",\n",
    "                    \"pubDate\": \"ISO 8601形式の日時\",\n",
    "                    \"description\": \"本文\",\n",
    "                    \"link\": \"URL\"\n",
    "                }\n",
    "            ],\n",
    "            \"summary\": {\n",
    "                \"total_count\": 総記事数,\n",
    "                \"filtered_count\": フィルタ後記事数,\n",
    "                \"from_date\": \"ISO 8601形式\",\n",
    "                \"to_date\": \"ISO 8601形式\"\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # ヘルパー関数: キーワード照合（大文字小文字を区別しない）\n",
    "    def contains_keyword(text: str, keywords: List[str]) -> bool:\n",
    "        \"\"\"テキストにキーワードリストからの1つ以上が含まれるかチェック\"\"\"\n",
    "        if not text:\n",
    "            return False\n",
    "        text_lower = text.lower()\n",
    "        return any(kw.lower() in text_lower for kw in keywords)\n",
    "    \n",
    "    # ヘルパー関数: RFC 2822日付をISO 8601に変換\n",
    "    def parse_pub_date(date_str: str) -> tuple:\n",
    "        \"\"\"\n",
    "        RFC 2822形式の日付文字列をパースし、datetime と ISO文字列のペアを返す。\n",
    "        パース失敗時は (None, None) を返す。\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dt = parsedate_to_datetime(date_str)\n",
    "            return dt, dt.isoformat()\n",
    "        except (TypeError, ValueError):\n",
    "            return None, None\n",
    "    \n",
    "    # XMLファイルを取得（複数ファイル対応）\n",
    "    xml_files = sorted(data_dir.glob('*.xml'))\n",
    "    total_count = 0\n",
    "    articles_dict = {}  # guid をキーとした重複排除用辞書\n",
    "    \n",
    "    # 各XMLファイルをパース\n",
    "    for xml_file in xml_files:\n",
    "        try:\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # RSS 2.0の<item>要素を抽出\n",
    "            for item in root.findall('.//item'):\n",
    "                # 必須フィールドを取得\n",
    "                guid_elem = item.find('guid')\n",
    "                title_elem = item.find('title')\n",
    "                pub_date_elem = item.find('pubDate')\n",
    "                desc_elem = item.find('description')\n",
    "                link_elem = item.find('link')\n",
    "                \n",
    "                # 基本的なデータ抽出\n",
    "                guid = guid_elem.text if guid_elem is not None else None\n",
    "                title = title_elem.text if title_elem is not None else \"\"\n",
    "                description = desc_elem.text if desc_elem is not None else \"\"\n",
    "                link = link_elem.text.strip() if link_elem is not None and link_elem.text else \"\"\n",
    "                \n",
    "                total_count += 1\n",
    "                \n",
    "                # guid がない場合はスキップ\n",
    "                if not guid:\n",
    "                    continue\n",
    "                \n",
    "                # 日付をパースして期間フィルタ\n",
    "                pub_dt, pub_date_iso = parse_pub_date(\n",
    "                    pub_date_elem.text if pub_date_elem is not None else \"\"\n",
    "                )\n",
    "                if pub_dt is None or not (from_date <= pub_dt <= to_date):\n",
    "                    continue\n",
    "                \n",
    "                # キーワード照合：トピック AND リージョン\n",
    "                combined_text = f\"{title} {description}\"\n",
    "                if not contains_keyword(combined_text, topic_keywords):\n",
    "                    continue\n",
    "                if not contains_keyword(combined_text, region_keywords):\n",
    "                    continue\n",
    "                \n",
    "                # guid でユニーク化（重複時は後出を保持）\n",
    "                articles_dict[guid] = {\n",
    "                    \"guid\": guid,\n",
    "                    \"title\": title,\n",
    "                    \"pubDate\": pub_date_iso,\n",
    "                    \"description\": description,\n",
    "                    \"link\": link\n",
    "                }\n",
    "                \n",
    "        except ET.ParseError:\n",
    "            # XMLパースエラーは無視して次ファイルへ\n",
    "            continue\n",
    "    \n",
    "    # pubDate 降順（新しい順）でソート\n",
    "    articles_list = sorted(\n",
    "        articles_dict.values(),\n",
    "        key=lambda x: x[\"pubDate\"],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # 結果を構築\n",
    "    result = {\n",
    "        \"articles\": articles_list,\n",
    "        \"summary\": {\n",
    "            \"total_count\": total_count,\n",
    "            \"filtered_count\": len(articles_list),\n",
    "            \"from_date\": from_date.isoformat(),\n",
    "            \"to_date\": to_date.isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d1826",
   "metadata": {},
   "source": [
    "## 2. 関数の実行\n",
    "\n",
    "パラメータを編集して関数を実行します。このセルを編集することで、実行時の値を調整できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ設定\n",
    "data_dir_path = Path('data/01_raw')\n",
    "from_date = datetime(2025, 7, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "to_date = datetime(2025, 12, 31, 23, 59, 59, tzinfo=timezone.utc)\n",
    "\n",
    "topic_keywords = [\n",
    "    'Bedrock', 'SageMaker', 'Claude', 'LLM', 'foundation model', 'generative AI',\n",
    "    'embedding', 'fine-tuning', 'inference', 'RAG', 'prompt', 'model training',\n",
    "    'neural network', 'transformer', 'deep learning', 'machine learning'\n",
    "]\n",
    "\n",
    "region_keywords = [\n",
    "    'Tokyo', 'ap-northeast-1', 'Asia Pacific (Tokyo)', 'available in Tokyo',\n",
    "    'Tokyo region', 'all regions', 'all aws regions'\n",
    "]\n",
    "\n",
    "# 関数実行\n",
    "result = extract_aws_articles(\n",
    "    data_dir=data_dir_path,\n",
    "    from_date=from_date,\n",
    "    to_date=to_date,\n",
    "    topic_keywords=topic_keywords,\n",
    "    region_keywords=region_keywords\n",
    ")\n",
    "\n",
    "# 結果を JSON 形式で出力\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0d068",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ 実行ガイド\n",
    "\n",
    "実行可能なセルを上から順番に実行すると、エージェントがユーザーからのリクエストを解決するために作成した関数を試行できます。\n",
    "\n",
    "1. **関数定義**: セル1を実行して関数を定義\n",
    "2. **関数実行**: セル2のパラメータを編集してから実行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbad0a7f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 開発者向けガイド\n",
    "\n",
    "## Agentの再実行\n",
    "\n",
    "ソースコードを変更した場合は、以下のコマンドでAgentを再実行してください：\n",
    "\n",
    "```bash\n",
    "# トレース付きで実行\n",
    "BYPASS_TOOL_CONSENT=true uv run python src/main.py\n",
    "```\n",
    "\n",
    "実行すると以下が自動生成されます：\n",
    "- `outputs/trace.jsonl` - 完全な実行トレース\n",
    "- `outputs/agent_replay.ipynb` - このノートブック\n",
    "\n",
    "### トレースファイルの確認\n",
    "\n",
    "`outputs/trace.jsonl` には完全な実行トレースがJSON形式で保存されています。\n",
    "各オブジェクトは OpenTelemetry の Span を表し、以下の情報が含まれます：\n",
    "\n",
    "- `gen_ai.user.message`: ユーザーからのプロンプト\n",
    "- `gen_ai.choice`: LLMの応答（テキストまたはツール使用）\n",
    "- `gen_ai.tool.name`: 使用されたツール名\n",
    "- `gen_ai.usage.*`: トークン使用量\n",
    "- `gen_ai.event.start_time/end_time`: 実行時刻"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
